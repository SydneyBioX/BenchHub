---
title: "Introduction of benchmarkInsight class"
output: BiocStyle::html_document
vignette: >
  %\VignetteIndexEntry{Overview of scFeatures with case studies}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE, warning=FALSE)
```

```{r}
devtools::load_all()
library(BenchHub)
library(readr)
library(dplyr)
library(stringr)
```

# Motivation

BenchHubâ€”an R ecosystem to make benchmarking easier. It organizes
evaluation metrics, gold standards(Auxiliary Data), and even provides
built-in visualization tools to help interpret results. With BenchHub,
researchers can quickly compare new methods, gain insights, and actually
trust their benchmarking studies. In this vignette, we are going to
introduce benchmarkInsights class.

# Creating benchmarkInsights class

`benchmarkInsight` objects can be created using the corresponding
constructor. For example, if you have a benchmark result formatted in
dataframe, you can create a `benchmarkInsight` object as follows. The
dataframe includes fixed name columns: `datasetID`, method, auxData,
metric, result. Here I will use the benchmark result from
SpatialSimBench to create a new object.

`benchmarkInsight` object can be instantiated using their respective
constructors. For example, if you have a benchmark result stored as a
dataframe, you can create a Trio object as follows. The dataframe must
include the following fixed columns: `datasetID`, `method`, `auxData`,
`metric`, and `result`. Here, I demonstrate this using benchmark results
from SpatialSimBench to initialize a new object.

```{r}
result_path <- system.file("extdata", "spatialsimbench_result.csv", package = "BenchHub")
# spatialsimbench_result <- read_csv(result_path)
spatialsimbench_result <-  read_csv("spatialsimbenchV1.csv")
spatialsimbench_result <- spatialsimbench_result %>%
  mutate(auxData = ifelse(metric %in% c("time", "memory"), 
                          as.numeric(sub("_.*", "", auxData)) * as.numeric(sub(".*_", "", auxData)), 
                          auxData))

glimpse(spatialsimbench_result)
```

If you use `trio$evaluation()`, the output will be automatically
formatted as the required dataframe. However, if you use your own
benchmark evaluation results, you ensure they adhere to the expected
format.

```{r}
bmi <- benchmarkInsights$new(spatialsimbench_result)
bmi
```

If you have additional evaluation result, you can use
`addevalSummary()`. Here is the example:

```{r}
add_result <- data.frame(
  datasetID = rep("BREAST", 13),
  method = c("scDesign2", "scDesign3_gau", "scDesign3_nb", "scDesign3_poi", 
             "SPARsim", "splatter", "SRTsim", "symsim", "zinbwave", 
             "scDesign3_gau(rf)", "scDesign3_nb(rf)", "scDesign3_poi(rf)", "SRTsim(rf)"),
  auxData = rep("svg", 13),
  metric = rep("recall", 13),
  result = c(0.921940928, 0.957805907, 0.964135021, 0.989451477, 0.774261603, 
             0.890295359, 0.985232068, 0.067510549, 0.888185654, 
             0.957805907, 0.964135021, 0.989451477, 0.985232068),
  stringsAsFactors = FALSE
)

bmi$addevalSummary(add_result)
```

If you add additional metadata of method, you can use `addMetadata()`.
Here is the example:

```{r}
metadata_srtsim <- data.frame(
  method = "SRTsim",
  year = 2023,
  packageVersion = "0.99.6",
  parameterSetting = "default",
  spatialInfoReq = "No",
  DOI = "10.1186/s13059-023-02879-z",
  stringsAsFactors = FALSE
  )

bmi$addMetadata(metadata_srtsim)
```

# Visualization

## Available plot

`getHeatmap(evalReuslt)`: Creates a heatmap from the evaluation summary
by averaging results across datasets.

-   evalResult: A dataframe containing the evaluation summary.
-   Note: In this heatmap, it averages results across datasets.

`getCorplot(evalReuslt, input_type)`: Creates a correlation plot
based on the provided evaluation summary.

-   evalResult: A dataframe containing the evaluation summary.
-   input_type: either "auxData", "metric", or "method".

`getBoxplot(evalReuslt)`: Creates a boxplot based on the provided
evaluation summary.

-   evalReuslt: A dataframe containing the evaluation summary.
-   input_type: either "auxData", "metric", or "method".

`getForestplot(evalReuslt, input_group, input_model)`: Create a
forest plot using linear models based on the comparison between groups
in the provided evaluation summary.

-   evalReuslt: A dataframe containing the evaluation summary.
-   input_group: A string specifying the grouping variable (only
    "datasetID", "method", or "auxData" allowed).
-   input_model: A string specifying the model variable (only
    "datasetID", "method", or "auxData" allowed).

`getScatterplot(evalReuslt, variables)`: a scatter plot for the
same auxData, with an two methodd metrics.

-   evalReuslt: A dataframe containing the evaluation summary, only include two
    different metrics, all auxData should be same
-   variables: A character vector of length two specifying the metric
    names to be used for the x and y axes.

`getLineplot(evalReuslt, order)`: Creates a line plot for the given
x and y variables, with an optional grouping and fixed x order.

-   evalReuslt: A dataframe containing the evaluation summary.
-   order: An optional vector specifying the order of x-axis values.

## Interpretation benchmark result

### Case Study: What is the overview of summary?

To get a high-level view of method performance, we use a heatmap to
summarize evaluation results across datasets. This helps identify
overall trends, making it easier to compare methods and performance
differences.

```{r}
bmi$getHeatmap(bmi$evalSummary)
```

### Case Study: What is the correlation between auxData/metric/method?

To understand the relationships between different evaluation factors, we
use a correlation plot to examine how auxData, metrics, and methods are
interrelated. This helps identify patterns, redundancies, or
dependencies among evaluation components.

```{r}
bmi$getCorplot(bmi$evalSummary, "method")
```

To further investigate the relationship between two specific metrics, we
use a scatter plot. This visualization helps assess how well two metrics
align or diverge across different methods, providing insights into
trade-offs and performance consistency.

```{r}
bmi$getScatterplot(bmi$evalSummary, c("recall","precision"))
```

### Case Study: What is the time and memory trend?

To evaluate the scalability of different methods, we use a line plot to
visualize trends in computational time and memory usage across different
conditions. This helps identify how methods perform as data complexity
increases, revealing potential efficiency trade-offs.

```{r}
bmi$getLineplot(bmi$evalSummary, "memory")
```

### Case Study: Which metric is most effective on the method?

To assess which metrics have the strongest influence on method
performance, we use a forest plot to visualize the relationship between
metrics and methods. This allows us to quantify and compare the impact
of different metrics, helping to identify the most critical evaluation
factors.

```{r}
bmi$getForestplot(bmi$evalSummary, "metric", "method")
```

### Case Study: How does method variability differ across datasets for a specific metric?

To examine the consistency of each method across different datasets for
a given metric, we use a boxplot. This visualization helps assess the
variability of method performance, highlighting robustness or
instability when applied to different datasets.

```{r}
bmi$getBoxplot(bmi$evalSummary, metricVariable = "KDEstat", auxDataVariable = "scaledVar")
```

## Cheatsheet

|              Question              | Code                                                       |
|:-----------------------:|:-----------------------------------------------|
|          Summary Overview          | `getHeatmap(evalReuslt)`                                   |
|        Correlation Analysis        | `getCorplot(evalReuslt, input_type)`                  |
|  Scalability Trend (Time/ Memory)  | `getLineplot(evalReuslt, order)`                      |
|   Metric-Model Impact (Modeling)   | `getForestplot(evalReuslt, input_group, input_model)` |
| Method Variability Across Datasets | `getBoxplot(evalReuslt)`                              |
|        Metric Relationship         | `getScatterplot(evalReuslt, variables)`               |
